{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU",
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 226037268,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents",
   "metadata": {
    "id": "1TFwaJir_Olj"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Environment Setup",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB.\n",
    "安装必要的python软件包，并下载LLaMA 3.1 8B量化版本的模型权重。另外，下载数据集。请注意，模型重量约为8GB。"
   ],
   "metadata": {
    "id": "mGx000oZ_Oll"
   }
  },
  {
   "cell_type": "code",
   "source": "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt",
   "metadata": {
    "id": "5JywoPOO_Oll"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import torch\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')",
   "metadata": {
    "id": "kX6SizAt_Olm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Prepare the LLM and LLM utility function 准备 LLM 和 LLM 效用函数",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models. \n",
    "默认情况下，我们将使用LLaMA 3.1 8B的量化版本，您可以通过使用提供的 LLM 和 LLM 效用函数来获得此作业的满分。您还可以尝试不同的 LLM 型号。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "T59vxAo2_Olm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily.\n",
    "在下面的代码块中，我们将首先将下载的 LLM 模型权重加载到GPU上。然后，我们实现了generate_response() 函数，以便您可以更轻松地从 LLM 模型获取生成的响应。"
   ],
   "metadata": {
    "id": "vtepTeT3_Olm"
   }
  },
  {
   "cell_type": "markdown",
   "source": "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning.",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   }
  },
  {
   "cell_type": "code",
   "source": "from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    This function will inference the model with given messages.\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,    # This argument is how many tokens the model can generate.\n        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output",
   "metadata": {
    "id": "ScyW45N__Olm"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Search Tool",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections.\n",
    "TA已经实现了一个搜索工具，您可以使用Google搜索来搜索某些关键字。您可以使用此工具搜索给定问题的相关网页。搜索工具可以集成在以下各节中。"
   ],
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ],
   "metadata": {
    "id": "bEIRmZl7_Oln"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Test the LLM inference pipeline",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   }
  },
  {
   "cell_type": "code",
   "source": "# You can try out different questions here.\ntest_question='請問誰是 Taylor Swift？'\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n    {\"role\": \"user\", \"content\": test_question}, # User prompt\n]\n\nprint(generate_response(llama3, messages))",
   "metadata": {
    "id": "8dmGCARd_Oln"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Agents",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "TA已经为您实现了Agent类。可以使用此类创建可与 LLM 模型交互的代理。Agent类具有以下属性和方法:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "    - role_description: 代理的角色。例如，如果您希望此代理成为历史专家，则可以将role_description设置为 “您是历史专家。您只会根据过去的真实情况回答问题。如果您没有可靠的来源，请不要生成任何答案。”。\n",
    "    - task_description: 代理的任务。例如，如果您希望此座席仅在 “是/否” 中回答问题，则可以将task_description设置为 “请在“ 是/否 ”中回答以下问题。不需要说明。”\n",
    "    - llm: 只是代理使用的 LLM 模型的指示符。\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output.\n",
    "    - 推理: 此方法将消息作为输入，并返回从 LLM 模型生成的响应。该消息将首先被格式化为 LLM 模型的适当输入。(在这里您可以设置一些全局说明，例如 “请以礼貌的方式发言” 或 “请提供详细说明”。)生成的响应将作为输出返回。"
   ],
   "metadata": {
    "id": "HGsIPud3_Oln"
   }
  },
  {
   "cell_type": "code",
   "source": "class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n        self.task_description = task_description    # Task description instructs what task should this agent solve.\n        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n    def inference(self, message:str) -> str:\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n            # TODO: Design the system prompt and user prompt here.\n            # Format the messsages first.\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n            return \"\"",
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "TODO 1: Design the role description and task description for each agent.\n",
    "       设计每个角色的描述和任务描述。"
   ],
   "metadata": {
    "id": "0-ueJrgP_Oln"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=(\n",
    "        \"你是一名专业的信息分析师，专注于从用户的问题中提取核心问题。\"\n",
    "        \"你的任务是过滤掉问题中的冗余信息、背景描述和无关细节，仅保留问题的核心诉求。\"\n",
    "    ),\n",
    "    task_description=(\n",
    "        \"请仔细分析用户的问题，去除所有不必要的背景信息和重复描述，\"\n",
    "        \"仅返回问题的核心部分。例如：将'我最近在研究机器学习，想了解卷积神经网络在图像识别中的具体应用'\"\n",
    "        \"简化为'卷积神经网络在图像识别中的具体应用'。\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=(\n",
    "        \"你是一名专业的信息检索专家，擅长从问题中提取关键概念和实体。\"\n",
    "        \"你的任务是识别问题中与答案密切相关的关键词和短语。\"\n",
    "    ),\n",
    "    task_description=(\n",
    "        \"请从用户的问题中提取3-5个最重要的关键词或短语，用逗号分隔列出。\"\n",
    "        \"关键词应包含问题的核心概念、实体名称、技术术语等。\"\n",
    "        \"例如：问题'如何用Python实现K-means聚类算法'应提取'Python,K-means聚类算法,实现方法'\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題，保持专业严谨的语气，用外部资料时需明确标注来源并确保信息准确性。\",\n",
    "    task_description=\"请根据以下问题和提供的参考资料，生成完整、准确且符合语法规范的回答：\",\n",
    ")"
   ],
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## RAG pipeline",
   "metadata": {
    "id": "A9eoywr7_Oln"
   }
  },
  {
   "cell_type": "markdown",
   "source": "TODO 2: Implement the RAG pipeline.",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!\n",
    "此外，可能有更多的启发式 (例如，基于问题的长度对问题进行分类，确定问题是否需要搜索，在将答案返回给用户之前重新确认答案……)，这些没有在流程图中示出。你可以用你的创造力来想出一个更好的解决方案!"
   ],
   "metadata": {
    "id": "MRGNa-1i_Oln"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- Naive approach (simple baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- Naive RAG approach (medium baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)",
   "metadata": {
    "id": "mppO-oOO_Olo"
   }
  },
  {
   "cell_type": "markdown",
   "source": "- RAG with agents (strong baseline)\n\n    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)",
   "metadata": {
    "id": "HYxbciLO_Olo"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # TODO: Implement your pipeline.\n",
    "    # Currently, it only feeds the question directly to the LLM. 目前，它仅将问题直接提供给 LLM\n",
    "    # You may want to get the final results through multiple inferences. 您可能希望通过多个推论获得最终结果\n",
    "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
    "    # 确保您的输入长度在模型上下文窗口的限制范围内 (16384token)\n",
    "    \n",
    "    try:\n",
    "        # 1. 提取问题核心\n",
    "        core_question = question_extraction_agent.inference(question).strip()\n",
    "        \n",
    "        # 2. 提取关键词（处理可能的多行输出）\n",
    "        keywords = keyword_extraction_agent.inference(core_question).strip()\n",
    "        keywords = keywords.replace('\\n', ',').replace(' ', '')  # 去除空格和换行\n",
    "        \n",
    "        # 3. 执行搜索（处理可能的空结果）\n",
    "        search_results = await search(keywords, 3)\n",
    "        context = ' '.join(search_results[:3]) if search_results else ''\n",
    "        context = context[:15000]  # 确保不超过上下文限制\n",
    "        \n",
    "        # 4. 构建完整输入（注意格式）\n",
    "        full_prompt = (\n",
    "            f\"用户问题：{question}\\n\"\n",
    "            f\"核心问题：{core_question}\\n\"\n",
    "            f\"参考资料：{context}\\n\"\n",
    "            f\"请根据以上信息，用繁体中文生成完整回答：\"\n",
    "        )\n",
    "        \n",
    "        # 5. 生成最终答案（处理过长输入）\n",
    "        if len(full_prompt) > 16000:\n",
    "            # 截断策略：优先保留核心问题和最新搜索结果\n",
    "            context = context[:14000]\n",
    "            full_prompt = (\n",
    "                f\"用户问题：{question[:1000]}\\n\"\n",
    "                f\"核心问题：{core_question[:2000]}\\n\"\n",
    "                f\"参考资料：{context}\\n\"\n",
    "                f\"请根据以上信息，用繁体中文生成完整回答：\"\n",
    "            )\n",
    "        \n",
    "        return qa_agent.inference(full_prompt)\n",
    "    \n",
    "    except Exception as e:\n",
    "        # 备用方案：直接返回基础回答\n",
    "        print(f\"Pipeline error: {str(e)}\")\n",
    "        return qa_agent.inference(question)\n",
    "    \n",
    "    # return qa_agent.inference(question)"
   ],
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Answer the questions using your pipeline!",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process.\n",
    "由于Colab有使用限制，您可能会遇到断开连接。以下代码将保存每个问题的答案。如果您按照指示安装了Google云端硬盘，则可以重新运行整个笔记本以继续您的过程。"
   ],
   "metadata": {
    "id": "PN17sSZ8DUg7"
   }
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\n\n# Fill in your student ID first.\nSTUDENT_ID = \"\"\n\nSTUDENT_ID = STUDENT_ID.lower()\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]\n    for id, question in enumerate(questions, 1):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n            print(answer, file=output_f)",
   "metadata": {
    "id": "plUDRTi_B39S"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Combine the results into one file.\nwith open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n    for id in range(1,91):\n        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)",
   "metadata": {
    "id": "GmLO9PlmEBPn"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
